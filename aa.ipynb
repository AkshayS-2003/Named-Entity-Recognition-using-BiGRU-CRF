{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d352121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1050795, 24)\n",
      "['lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos', 'next-next-shape', 'next-next-word', 'next-pos', 'next-shape', 'next-word', 'pos', 'prev-iob', 'prev-lemma', 'prev-pos', 'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape', 'prev-prev-word', 'prev-shape', 'prev-word', 'sentence_idx', 'shape', 'word', 'tag']\n",
      "      lemma next-lemma next-next-lemma next-next-pos next-next-shape  \\\n",
      "0  thousand         of        demonstr           NNS       lowercase   \n",
      "1        of   demonstr            have           VBP       lowercase   \n",
      "2  demonstr       have           march           VBN       lowercase   \n",
      "3      have      march         through            IN       lowercase   \n",
      "4     march    through          london           NNP     capitalized   \n",
      "\n",
      "  next-next-word next-pos next-shape      next-word  pos  ... prev-prev-lemma  \\\n",
      "0  demonstrators       IN  lowercase             of  NNS  ...      __start2__   \n",
      "1           have      NNS  lowercase  demonstrators   IN  ...      __start1__   \n",
      "2        marched      VBP  lowercase           have  NNS  ...        thousand   \n",
      "3        through      VBN  lowercase        marched  VBP  ...              of   \n",
      "4         London       IN  lowercase        through  VBN  ...        demonstr   \n",
      "\n",
      "  prev-prev-pos prev-prev-shape prev-prev-word   prev-shape      prev-word  \\\n",
      "0    __START2__        wildcard     __START2__     wildcard     __START1__   \n",
      "1    __START1__        wildcard     __START1__  capitalized      Thousands   \n",
      "2           NNS     capitalized      Thousands    lowercase             of   \n",
      "3            IN       lowercase             of    lowercase  demonstrators   \n",
      "4           NNS       lowercase  demonstrators    lowercase           have   \n",
      "\n",
      "  sentence_idx        shape           word tag  \n",
      "0          1.0  capitalized      Thousands   O  \n",
      "1          1.0    lowercase             of   O  \n",
      "2          1.0    lowercase  demonstrators   O  \n",
      "3          1.0    lowercase           have   O  \n",
      "4          1.0    lowercase        marched   O  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"Dataset/ner.csv\", sep=\",\", encoding=\"latin1\", engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "# Drop the unwanted unnamed column if it exists\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83a85eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 35177\n",
      "First sentence example: [('Thousands', 'O'), ('of', 'O'), ('demonstrators', 'O'), ('have', 'O'), ('marched', 'O'), ('through', 'O'), ('London', 'B-geo'), ('to', 'O'), ('protest', 'O'), ('the', 'O')]\n"
     ]
    }
   ],
   "source": [
    "train_sents = []\n",
    "for idx, group in df.groupby(\"sentence_idx\"):   # change if your column name is different\n",
    "    sentence = []\n",
    "    for _, row in group.iterrows():\n",
    "        word = str(row[\"word\"]) if pd.notna(row[\"word\"]) else \"\"   # safe string conversion\n",
    "        tag = row[\"tag\"] if pd.notna(row[\"tag\"]) else \"PAD\"\n",
    "        sentence.append((word, tag))   # append only once\n",
    "    train_sents.append(sentence)\n",
    "\n",
    "print(\"Number of sentences:\", len(train_sents))\n",
    "print(\"First sentence example:\", train_sents[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1ab041b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char vocab size: 98\n"
     ]
    }
   ],
   "source": [
    "# Create character vocabulary\n",
    "all_chars = {c for sent in train_sents for word, tag in sent for c in word}\n",
    "\n",
    "char2idx = {\"PAD\": 0, \"UNK\": 1}\n",
    "char2idx.update({c: i for i, c in enumerate(sorted(all_chars), start=2)})\n",
    "\n",
    "print(\"Char vocab size:\", len(char2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f8fa9992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sentences: 28141\n",
      "Test sentences: 7036\n",
      "Vocab size: 30173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train_sents is your full list of sentences (word, tag pairs)\n",
    "train_sents_train, train_sents_test = train_test_split(\n",
    "    train_sents, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Now collect all unique words from both splits\n",
    "all_words_from_train_and_test = set(\n",
    "    [str(w) for sent in train_sents_train + train_sents_test for w, t in sent]\n",
    ")\n",
    "\n",
    "\n",
    "# Build word2idx mapping\n",
    "word2idx = {\"PAD\": 0, \"UNK\": 1}\n",
    "word2idx.update({w: i+2 for i, w in enumerate(sorted(all_words_from_train_and_test))})\n",
    "\n",
    "print(\"Training sentences:\", len(train_sents_train))\n",
    "print(\"Test sentences:\", len(train_sents_test))\n",
    "print(\"Vocab size:\", len(word2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f7d00724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_words: (35177, 140)\n",
      "X_chars: (35177, 140, 15)\n",
      "y_tags: (35177, 140)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = max(len(s) for s in train_sents)\n",
    "max_word_len = 15  # max chars per word\n",
    "\n",
    "# Build word2idx and tag2idx\n",
    "words = sorted(set([str(w) for sent in train_sents for w, t in sent]))\n",
    "word2idx = {\"PAD\": 0, \"UNK\": 1}\n",
    "word2idx.update({w: i+2 for i, w in enumerate(words)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tags = sorted(set(df[\"tag\"].dropna().values))\n",
    "tag2idx = {\"PAD\": 0}\n",
    "for i, t in enumerate(tags, start=1):\n",
    "    tag2idx[t] = i\n",
    "\n",
    "# Encode one sentence\n",
    "def encode_sentence(sent, word2idx, tag2idx, char2idx, max_len=max_len, max_word_len=max_word_len):\n",
    "    word_ids, char_ids, tag_ids = [], [], []\n",
    "    \n",
    "    for word, tag in sent[:max_len]:\n",
    "        word_ids.append(word2idx.get(word, word2idx[\"UNK\"]))\n",
    "        tag_ids.append(tag2idx.get(tag, tag2idx[\"PAD\"]))\n",
    "        \n",
    "        # char-level indices\n",
    "        chars = [char2idx.get(c, char2idx[\"UNK\"]) for c in word[:max_word_len]]\n",
    "        # pad chars\n",
    "        while len(chars) < max_word_len:\n",
    "            chars.append(char2idx[\"PAD\"])\n",
    "        char_ids.append(chars)\n",
    "    \n",
    "    # pad sentence-level\n",
    "    while len(word_ids) < max_len:\n",
    "        word_ids.append(word2idx[\"PAD\"])\n",
    "        tag_ids.append(tag2idx[\"PAD\"])\n",
    "        char_ids.append([char2idx[\"PAD\"]] * max_word_len)\n",
    "    \n",
    "    return word_ids, char_ids, tag_ids\n",
    "\n",
    "# Encode all sentences\n",
    "X_words, X_chars, y_tags = [], [], []\n",
    "for sent in train_sents:\n",
    "    w, c, t = encode_sentence(sent, word2idx, tag2idx, char2idx)\n",
    "    X_words.append(w)\n",
    "    X_chars.append(c)\n",
    "    y_tags.append(t)\n",
    "\n",
    "import numpy as np\n",
    "X_words = np.array(X_words)\n",
    "X_chars = np.array(X_chars)\n",
    "y_tags  = np.array(y_tags)\n",
    "\n",
    "print(\"X_words:\", X_words.shape)\n",
    "print(\"X_chars:\", X_chars.shape)\n",
    "print(\"y_tags:\", y_tags.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8c1fe5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 30172\n",
      "Number of tags: 18\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Extract words and tags from dataset\n",
    "words = list(set(df[\"word\"].dropna().values))   # dropna() ensures no NaN values\n",
    "tags = sorted(set(df[\"tag\"].dropna().values))   # sorted for consistency\n",
    "\n",
    "# Create word2idx mapping (+2 for PAD and UNK)\n",
    "word2idx = {w: i + 2 for i, w in enumerate(words)}\n",
    "word2idx[\"PAD\"] = 0\n",
    "word2idx[\"UNK\"] = 1\n",
    "\n",
    "# Create tag2idx mapping\n",
    "tag2idx = {\"PAD\": 0}\n",
    "tag2idx.update({t: i+1 for i, t in enumerate(tags)})\n",
    "\n",
    "# Reverse mapping for decoding predictions\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "print(\"Vocab size:\", len(word2idx))\n",
    "print(\"Number of tags:\", len(tag2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fea64746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length: 140\n",
      "Embedding vocab size: 30172\n",
      "X_train shape: (28141, 140) Max index in X_train: 30172\n",
      "X_test shape: (7036, 140) Max index in X_test: 30172\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure PAD and UNK exist in word2idx\n",
    "if \"PAD\" not in word2idx:\n",
    "    word2idx[\"PAD\"] = len(word2idx)\n",
    "if \"UNK\" not in word2idx:\n",
    "    word2idx[\"UNK\"] = len(word2idx)\n",
    "\n",
    "# Split sentences\n",
    "train_sents_train, train_sents_test = train_test_split(train_sents, test_size=0.2, random_state=42)\n",
    "\n",
    "# Maximum sentence length for padding\n",
    "max_len = max(len(s) for s in train_sents_train + train_sents_test)\n",
    "print(\"Max sentence length:\", max_len)\n",
    "\n",
    "# Encode word indices\n",
    "X_train = [[word2idx.get(w, word2idx[\"UNK\"]) for w, t in s] for s in train_sents_train]\n",
    "X_test  = [[word2idx.get(w, word2idx[\"UNK\"]) for w, t in s] for s in train_sents_test]\n",
    "\n",
    "# Pad sequences to max_len\n",
    "X_train = pad_sequences(X_train, maxlen=max_len, padding='post', value=word2idx[\"PAD\"])\n",
    "X_test  = pad_sequences(X_test,  maxlen=max_len, padding='post', value=word2idx[\"PAD\"])\n",
    "\n",
    "print(\"Embedding vocab size:\", len(word2idx))\n",
    "print(\"X_train shape:\", X_train.shape, \"Max index in X_train:\", X_train.max())\n",
    "print(\"X_test shape:\", X_test.shape, \"Max index in X_test:\", X_test.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73130fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch  # \n",
    "\n",
    "def load_glove_embeddings(glove_path, word2idx, embedding_dim=100):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_path, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = vector\n",
    "\n",
    "    #  Add +1 to handle max index properly\n",
    "    embedding_matrix = np.zeros((len(word2idx) + 1, embedding_dim))\n",
    "\n",
    "    for word, idx in word2idx.items():\n",
    "        vec = embeddings_index.get(word.lower())\n",
    "        if vec is not None:\n",
    "            embedding_matrix[idx] = vec\n",
    "\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa9688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: torch.Size([30173, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "glove_path = \"glove.6B.100d.txt\"  # make sure this file exists\n",
    "\n",
    "# Check if embeddings are loaded successfully\n",
    "embedding_matrix = load_glove_embeddings(glove_path, word2idx, embedding_dim=100)\n",
    "\n",
    "if embedding_matrix is not None:\n",
    "    print(\"Embedding matrix shape:\", embedding_matrix.shape)\n",
    "else:\n",
    "    print(\"Embedding matrix could not be created. Check GloVe path and word2idx.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "50174599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (35177, 140)\n",
      "y shape: (35177, 140)\n",
      "Vocab size: 30172\n",
      "Number of tags: 18\n",
      "Max index in X: 30172\n",
      "Max index in y: 17\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = max(len(s) for s in train_sents)\n",
    "\n",
    "# Ensure PAD and UNK tokens exist in vocab\n",
    "if \"PAD\" not in word2idx:\n",
    "    word2idx[\"PAD\"] = len(word2idx)\n",
    "if \"UNK\" not in word2idx:\n",
    "    word2idx[\"UNK\"] = len(word2idx)\n",
    "\n",
    "# Rebuild X with safe mapping\n",
    "X = [[word2idx.get(w, word2idx[\"UNK\"]) for w, t in s] for s in train_sents]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx[\"PAD\"])\n",
    "\n",
    "# Build tag dictionary with PAD\n",
    "tags = sorted(set(df[\"tag\"].dropna().values))\n",
    "tag2idx = {\"PAD\": 0}\n",
    "tag2idx.update({t: i+1 for i, t in enumerate(tags)})\n",
    "idx2tag = {i: t for t, i in tag2idx.items()}\n",
    "\n",
    "# Rebuild y\n",
    "y = [[tag2idx[t] for w, t in s] for s in train_sents]\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Vocab size:\", len(word2idx))\n",
    "print(\"Number of tags:\", len(tag2idx))\n",
    "print(\"Max index in X:\", X.max())\n",
    "print(\"Max index in y:\", y.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b6a1d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X, dtype=torch.long)\n",
    "y = torch.tensor(y, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6ebbcb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes -> Words: torch.Size([35177, 140]) Chars: torch.Size([35177, 140, 15]) Tags: torch.Size([35177, 140])\n"
     ]
    }
   ],
   "source": [
    "# Encode dataset into word, char, and tag ids\n",
    "X_words, X_chars, y_tags = [], [], []\n",
    "\n",
    "for sent in train_sents:  # <---- make sure you’re using train_sents\n",
    "    w, c, t = encode_sentence(sent, word2idx, tag2idx, char2idx, max_len=140, max_word_len=max_word_len)\n",
    "    X_words.append(w)\n",
    "    X_chars.append(c)\n",
    "    y_tags.append(t)\n",
    "\n",
    "X_words = torch.tensor(X_words, dtype=torch.long)\n",
    "X_chars = torch.tensor(X_chars, dtype=torch.long)   # no need for np.array\n",
    "y_tags  = torch.tensor(y_tags, dtype=torch.long)\n",
    "\n",
    "print(\"Shapes -> Words:\", X_words.shape, \"Chars:\", X_chars.shape, \"Tags:\", y_tags.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75df3645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: torch.Size([32, 140])\n",
      "Chars: torch.Size([32, 140, 15])\n",
      "Tags: torch.Size([32, 140])\n",
      "Mask: torch.Size([32, 140])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#  Make sure everything is numpy first\n",
    "X_words = np.array(X_words)\n",
    "X_chars = np.array(X_chars)   # (num_sents, seq_len, max_word_len)\n",
    "y_tags  = np.array(y_tags)\n",
    "\n",
    "# Dataset\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, X_words, X_chars, y_tags, pad_tag_idx=0):\n",
    "        self.words = torch.tensor(X_words, dtype=torch.long)\n",
    "        self.chars = torch.tensor(X_chars, dtype=torch.long)\n",
    "        self.tags  = torch.tensor(y_tags, dtype=torch.long)\n",
    "        self.pad_tag_idx = pad_tag_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word_ids = self.words[idx]\n",
    "        char_ids = self.chars[idx]\n",
    "        tag_ids  = self.tags[idx]\n",
    "\n",
    "        #  mask for real tokens only\n",
    "        mask = (tag_ids != self.pad_tag_idx)\n",
    "\n",
    "        return word_ids, char_ids, tag_ids, mask\n",
    "\n",
    "\n",
    "# Split numpy arrays first\n",
    "X_words_train, X_words_test, X_chars_train, X_chars_test, y_tags_train, y_tags_test = train_test_split(\n",
    "    X_words, X_chars, y_tags, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ✅ Create datasets (conversion to torch happens inside NERDataset)\n",
    "train_dataset = NERDataset(X_words_train, X_chars_train, y_tags_train, pad_tag_idx=tag2idx[\"PAD\"])\n",
    "test_dataset  = NERDataset(X_words_test,  X_chars_test,  y_tags_test,  pad_tag_idx=tag2idx[\"PAD\"])\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# 🔍 Check one batch\n",
    "for word_ids, char_ids, tag_ids, mask in train_loader:\n",
    "    print(\"Words:\", word_ids.shape)   # (B, seq_len)\n",
    "    print(\"Chars:\", char_ids.shape)   # (B, seq_len, max_word_len)\n",
    "    print(\"Tags:\", tag_ids.shape)     # (B, seq_len)\n",
    "    print(\"Mask:\", mask.shape)        # (B, seq_len)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "633018e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X.shape[0] == y.shape[0], \"Mismatch: number of sentences differs!\"\n",
    "assert X.shape[1] == y.shape[1], \"Mismatch: sequence lengths differ!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f0748a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shapes: torch.Size([31659, 140]) torch.Size([31659, 140])\n",
      "Test shapes: torch.Size([3518, 140]) torch.Size([3518, 140])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Test shapes:\", X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7d351130",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "assert X_train.shape[1] == y_train.shape[1] == X_test.shape[1] == y_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f719bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "class BiGRU_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, char_vocab_size, word_emb_dim, char_emb_dim, \n",
    "                 hidden_dim, char_hidden_dim, num_tags, pad_idx):\n",
    "        super(BiGRU_CRF, self).__init__()\n",
    "        \n",
    "        # Word embeddings\n",
    "        self.word_emb = nn.Embedding(vocab_size, word_emb_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Char embeddings + Char-level BiGRU\n",
    "        self.char_emb = nn.Embedding(char_vocab_size, char_emb_dim, padding_idx=pad_idx)\n",
    "        self.char_gru = nn.GRU(char_emb_dim, char_hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Dropout layer (⚡ Add this)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Word-level BiGRU\n",
    "        self.bigru = nn.GRU(word_emb_dim + 2*char_hidden_dim, hidden_dim, \n",
    "                            batch_first=True, bidirectional=True)\n",
    "\n",
    "        # Linear layer to tag space\n",
    "        self.fc = nn.Linear(2*hidden_dim, num_tags)\n",
    "\n",
    "        # CRF layer\n",
    "        self.crf = CRF(num_tags, batch_first=True)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, word_ids, char_ids):\n",
    "        \"\"\"\n",
    "        word_ids: (batch, seq_len)\n",
    "        char_ids: (batch, seq_len, word_len)\n",
    "        \"\"\"\n",
    "        # Word embeddings\n",
    "        word_emb = self.word_emb(word_ids)\n",
    "\n",
    "        # Char embeddings\n",
    "        batch_size, seq_len, word_len = char_ids.size()\n",
    "        char_ids = char_ids.long()\n",
    "\n",
    "        char_ids = char_ids.view(batch_size * seq_len, word_len)  # flatten\n",
    "        char_emb = self.char_emb(char_ids)  # (batch*seq_len, word_len, char_emb_dim)\n",
    "\n",
    "        _, char_hidden = self.char_gru(char_emb)  # last hidden states\n",
    "        char_repr = torch.cat([char_hidden[0], char_hidden[1]], dim=-1)  # (batch*seq_len, 2*char_hidden_dim)\n",
    "        char_repr = char_repr.view(batch_size, seq_len, -1)  # reshape back\n",
    "\n",
    "        # Combine word + char\n",
    "        embeddings = torch.cat([word_emb, char_repr], dim=-1)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        # Word-level BiGRU\n",
    "        gru_out, _ = self.bigru(embeddings)\n",
    "        gru_out = self.dropout(gru_out)\n",
    "\n",
    "        emissions = self.fc(gru_out)\n",
    "        return emissions\n",
    "\n",
    "    def loss(self, word_ids, char_ids, tags, mask):\n",
    "        emissions = self.forward(word_ids, char_ids)\n",
    "        return -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
    "\n",
    "    def predict(self, word_ids, char_ids, mask):\n",
    "        emissions = self.forward(word_ids, char_ids)\n",
    "        return self.crf.decode(emissions, mask=mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1c25cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BiGRU_CRF(\n",
    "    vocab_size=len(word2idx),\n",
    "    num_tags=len(tag2idx),\n",
    "    char_vocab_size=len(char2idx),\n",
    "    word_emb_dim=100,      # size of word embeddings\n",
    "    char_emb_dim=30,       # size of character embeddings\n",
    "    char_hidden_dim=50,    # hidden dim for char-level BiGRU\n",
    "    hidden_dim=128,        # BiGRU hidden size\n",
    "    pad_idx=word2idx[\"PAD\"]\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "438f12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ba2077cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max index in X_train: tensor(30172)\n",
      "Vocab size: 30172\n"
     ]
    }
   ],
   "source": [
    "print(\"Max index in X_train:\", X_train.max())\n",
    "print(\"Vocab size:\", len(word2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9e730d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset\n",
    "X_words_train, X_words_test, X_chars_train, X_chars_test, y_train, y_test = train_test_split(\n",
    "    X_words, X_chars, y_tags, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Convert to torch tensors and create mask\n",
    "train_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_words_train, dtype=torch.long),\n",
    "    torch.tensor(X_chars_train, dtype=torch.long),\n",
    "    torch.tensor(y_train, dtype=torch.long),\n",
    "    torch.tensor((y_train != tag2idx[\"PAD\"]), dtype=torch.bool)\n",
    ")\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(\n",
    "    torch.tensor(X_words_test, dtype=torch.long),\n",
    "    torch.tensor(X_chars_test, dtype=torch.long),\n",
    "    torch.tensor(y_test, dtype=torch.long),\n",
    "    torch.tensor((y_test != tag2idx[\"PAD\"]), dtype=torch.bool)\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3237d65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max char index in train: 96\n",
      "Max char index in test: 97\n",
      "Char vocab size: 98\n"
     ]
    }
   ],
   "source": [
    "print(\"Max char index in train:\", X_chars_train.max())\n",
    "print(\"Max char index in test:\", X_chars_test.max())\n",
    "print(\"Char vocab size:\", len(char2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "da218b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([32, 140])\n",
      "torch.Size([32, 140, 15])\n",
      "torch.Size([32, 140])\n",
      "torch.Size([32, 140])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(len(batch))  # should be 4\n",
    "for x in batch:\n",
    "    print(x.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e4bd6435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 31659\n",
      "Number of test samples: 3518\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you already have these prepared:\n",
    "# X_words, X_chars, y_tags, sentences (all same length)\n",
    "\n",
    "(\n",
    "    X_words_train, X_words_test,\n",
    "    X_chars_train, X_chars_test,\n",
    "    y_tags_train, y_tags_test\n",
    ") = train_test_split(\n",
    "    X_words, X_chars, y_tags,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Number of train samples:\", len(X_words_train))\n",
    "print(\"Number of test samples:\", len(X_words_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8465f260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_words_train shape: (28141, 140)\n",
      "X_words_test shape: (7036, 140)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Maximum sequence length for padding\n",
    "max_len = 140  \n",
    "\n",
    "# ✅ Convert words to indices with UNK fallback\n",
    "X_words_train = [\n",
    "    [word2idx.get(w, word2idx[\"UNK\"]) for w, t in sent]\n",
    "    for sent in train_sents_train  # <-- using the correct training sentences list\n",
    "]\n",
    "\n",
    "X_words_test = [\n",
    "    [word2idx.get(w, word2idx[\"UNK\"]) for w, t in sent]\n",
    "    for sent in train_sents_test   # <-- using the correct test sentences list\n",
    "]\n",
    "\n",
    "# ✅ Pad sequences\n",
    "X_words_train = pad_sequences(\n",
    "    X_words_train, maxlen=max_len, padding=\"post\", value=word2idx[\"PAD\"]\n",
    ")\n",
    "X_words_test = pad_sequences(\n",
    "    X_words_test, maxlen=max_len, padding=\"post\", value=word2idx[\"PAD\"]\n",
    ")\n",
    "\n",
    "# ✅ Debugging info\n",
    "print(\"X_words_train shape:\", X_words_train.shape)\n",
    "print(\"X_words_test shape:\", X_words_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "89dc2b55",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[132]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m mask = (tag_ids != tag2idx[\u001b[33m\"\u001b[39m\u001b[33mPAD\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     22\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m loss = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m loss.backward()\n\u001b[32m     25\u001b[39m optimizer.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mBiGRU_CRF.loss\u001b[39m\u001b[34m(self, word_ids, char_ids, tags, mask)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, word_ids, char_ids, tags, mask):\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     emissions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m -\u001b[38;5;28mself\u001b[39m.crf(emissions, tags, mask=mask, reduction=\u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mBiGRU_CRF.forward\u001b[39m\u001b[34m(self, word_ids, char_ids)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[33;03mword_ids: (batch, seq_len)\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[33;03mchar_ids: (batch, seq_len, word_len)\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Word embeddings\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m word_emb = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mword_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Char embeddings\u001b[39;00m\n\u001b[32m     41\u001b[39m batch_size, seq_len, word_len = char_ids.size()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AKSHAY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AKSHAY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AKSHAY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\AKSHAY\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mIndexError\u001b[39m: index out of range in self"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "n_epochs = 50\n",
    "patience = 5   # stop if no improvement for 5 epochs\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # --- Training ---\n",
    "    for word_ids, char_ids, tag_ids, _ in train_loader:  # ignore precomputed mask\n",
    "        word_ids = word_ids.to(device)\n",
    "        char_ids = char_ids.to(device)\n",
    "        tag_ids = tag_ids.to(device)\n",
    "\n",
    "        # recompute mask\n",
    "        mask = (tag_ids != tag2idx[\"PAD\"])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(word_ids, char_ids, tag_ids, mask=mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs}, Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        for word_ids, char_ids, tag_ids, _ in test_loader:\n",
    "            word_ids = word_ids.to(device)\n",
    "            char_ids = char_ids.to(device)\n",
    "            tag_ids = tag_ids.to(device)\n",
    "\n",
    "            # recompute mask\n",
    "            mask = (tag_ids != tag2idx[\"PAD\"])\n",
    "\n",
    "            predictions = model.predict(word_ids, char_ids, mask=mask)\n",
    "\n",
    "            for pred_seq, true_seq, mask_seq in zip(predictions, tag_ids, mask):\n",
    "                true_seq = true_seq[mask_seq].tolist()  # only real tokens\n",
    "                assert len(pred_seq) == len(true_seq)\n",
    "\n",
    "                total += len(true_seq)\n",
    "                correct += sum(p == t for p, t in zip(pred_seq, true_seq))\n",
    "\n",
    "        val_acc = correct / total if total > 0 else 0\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "        # --- Early Stopping ---\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(\"✅ Model improved. Saved checkpoint.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"⚠️ No improvement. Patience: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(\"⏹ Early stopping triggered.\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02346250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         art       0.12      0.10      0.11        40\n",
      "         eve       0.28      0.33      0.30        30\n",
      "         geo       0.82      0.84      0.83      3792\n",
      "         gpe       0.89      0.93      0.91      1608\n",
      "         nat       0.40      0.36      0.38        22\n",
      "         org       0.66      0.64      0.65      1989\n",
      "         per       0.71      0.70      0.70      1653\n",
      "         tim       0.86      0.84      0.85      2040\n",
      "\n",
      "   micro avg       0.79      0.79      0.79     11174\n",
      "   macro avg       0.59      0.59      0.59     11174\n",
      "weighted avg       0.79      0.79      0.79     11174\n",
      "\n",
      "Overall F1: 0.7888695341462325\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "model.eval()\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch, mask in test_loader:\n",
    "        X_batch, y_batch, mask = X_batch.to(device), y_batch.to(device), mask.to(device)\n",
    "        preds = model.predict(X_batch, mask)\n",
    "        for p, y, m in zip(preds, y_batch, mask):\n",
    "            # keep only non-PAD tokens\n",
    "            true = [idx2tag[idx.item()] for idx, mask_val in zip(y, m) if mask_val == 1]\n",
    "            pred = [idx2tag[idx] for idx, mask_val in zip(p, m) if mask_val == 1]\n",
    "            all_labels.append(true)\n",
    "            all_preds.append(pred)\n",
    "\n",
    "print(classification_report(all_labels, all_preds))\n",
    "print(\"Overall F1:\", f1_score(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "063d8819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(sentence_idx=0):\n",
    "    words = [w for w, t in train_sents[sentence_idx]]\n",
    "    true_tags = [t for w, t in train_sents[sentence_idx]]\n",
    "    X_sample = torch.tensor([X_test[sentence_idx]], dtype=torch.long).to(device)\n",
    "    mask = (X_sample != word2idx[\"PAD\"]).to(device)\n",
    "\n",
    "    pred_tags = model.predict(X_sample, mask)[0]\n",
    "    pred_tags = [idx2tag[idx] for idx in pred_tags]\n",
    "\n",
    "    for w, t, p in zip(words, true_tags, pred_tags):\n",
    "        print(f\"{w:15}  True: {t:5}  Pred: {p:5}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75d90de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"bigru_crf_ner.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9791677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Akash', 'O'), ('is', 'O'), ('working', 'O'), ('in', 'O'), ('Senscript', 'O'), ('located', 'O'), ('in', 'O'), ('Kochi', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "def predict_sentence(sentence):\n",
    "    # convert words to indices\n",
    "    x = [word2idx.get(w, word2idx[\"UNK\"]) for w in sentence]\n",
    "    x = torch.tensor([x], dtype=torch.long).to(device)\n",
    "\n",
    "    mask = (x != word2idx[\"PAD\"]).to(device)\n",
    "    preds = model.predict(x, mask)[0]\n",
    "\n",
    "    return list(zip(sentence, [idx2tag[idx] for idx in preds]))\n",
    "\n",
    "# Example usage\n",
    "print(predict_sentence([\"Akash\", \"is\", \"working\", \"in\", \"Senscript\", \"located\",\"in\",\"Kochi\", \".\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b5d91e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted NER tags:\n",
      "Akash           -> O\n",
      "is              -> O\n",
      "working         -> O\n",
      "in              -> O\n",
      "senscript       -> O\n",
      "technologies    -> O\n",
      "located         -> O\n",
      "in              -> O\n",
      "kochi           -> B-geo\n",
      "\n",
      "Predicted NER tags:\n",
      "exit            -> O\n"
     ]
    }
   ],
   "source": [
    "def predict_sentence(sentence, model, word2idx, idx2tag, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Predict NER tags for a given input sentence.\n",
    "    sentence: list of words (already tokenized).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Convert words to indices (UNK for unknown)\n",
    "    x = [word2idx.get(w, word2idx[\"UNK\"]) for w in sentence]\n",
    "    x = torch.tensor([x], dtype=torch.long).to(device)\n",
    "\n",
    "    # Mask (1 for real tokens, 0 for PAD)\n",
    "    mask = (x != word2idx[\"PAD\"]).to(device)\n",
    "\n",
    "    # Predict\n",
    "    preds = model.predict(x, mask)[0]\n",
    "    pred_tags = [idx2tag[idx] for idx in preds]\n",
    "\n",
    "    return list(zip(sentence, pred_tags))\n",
    "\n",
    "\n",
    "# 🔹 Example usage with user input\n",
    "while True:\n",
    "    user_input = input(\"\\nEnter a sentence (or 'quit' to exit): \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    # Very basic tokenization (split by spaces)\n",
    "    words = user_input.strip().split()\n",
    "    predictions = predict_sentence(words, model, word2idx, idx2tag, device)\n",
    "\n",
    "    print(\"\\nPredicted NER tags:\")\n",
    "    for w, t in predictions:\n",
    "        print(f\"{w:15} -> {t}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f00aaf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in mappings: dict_keys(['id2label', 'label2id'])\n",
      "id2label: <class 'dict'> entries: 17\n",
      "label2id: <class 'dict'> entries: 17\n",
      "Sample id2label: {0: '0', 1: '1', 2: '2', 3: '3', 4: '4'}\n",
      "Sample label2id: {'0': 0, '1': 1, '2': 2, '3': 3, '4': 4}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the mappings from the .pt file\n",
    "mappings = torch.load(\"label_mappings.pt\", map_location=\"cpu\")\n",
    "\n",
    "print(\"Keys in mappings:\", mappings.keys())\n",
    "\n",
    "# Extract actual mappings\n",
    "id2label = mappings.get(\"id2label\")\n",
    "label2id = mappings.get(\"label2id\")\n",
    "\n",
    "print(\"id2label:\", type(id2label), \"entries:\", len(id2label) if id2label else None)\n",
    "print(\"label2id:\", type(label2id), \"entries:\", len(label2id) if label2id else None)\n",
    "\n",
    "# Optional: inspect first few entries\n",
    "if id2label:\n",
    "    print(\"Sample id2label:\", dict(list(id2label.items())[:5]))\n",
    "if label2id:\n",
    "    print(\"Sample label2id:\", dict(list(label2id.items())[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35591d6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "A load persistent id instruction was encountered,\nbut no persistent_load function was specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mlabel_mappings.pt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     mappings = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Handle both dict and tuple/list cases\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mappings, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[31mUnpicklingError\u001b[39m: A load persistent id instruction was encountered,\nbut no persistent_load function was specified."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"label_mappings.pt\", \"rb\") as f:\n",
    "    mappings = pickle.load(f)\n",
    "\n",
    "# Handle both dict and tuple/list cases\n",
    "if isinstance(mappings, dict):\n",
    "    char2idx = mappings.get(\"char2idx\")\n",
    "    tag2idx = mappings.get(\"tag2idx\")\n",
    "    idx2tag = mappings.get(\"idx2tag\") or {v: k for k, v in tag2idx.items()}\n",
    "elif isinstance(mappings, (tuple, list)):\n",
    "    if len(mappings) == 3:\n",
    "        char2idx, tag2idx, idx2tag = mappings\n",
    "    elif len(mappings) == 2:\n",
    "        char2idx, tag2idx = mappings\n",
    "        idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected mappings length: {len(mappings)}\")\n",
    "else:\n",
    "    raise ValueError(\"Unsupported format in label_mappings.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
